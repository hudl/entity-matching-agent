{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a13e54-75df-4401-a865-6899bf212ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U requests langchain langchain-aws langchain-community pydantic boto3 tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb0ab0-bb0f-46ee-9e0a-b5fc7130e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- LangChain Imports ---\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "HUDL_API_KEY = \"your_hudl_auth_key_here\"  # Replace with your actual API key\n",
    "GRAPHQL_ENDPOINT = \"https://master.thorhudl.com/api/graphql/query\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {HUDL_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# ---Step 1: Tool Definitions ---\n",
    "# --- Tool 1: Get Entity to be Merged ---\n",
    "class MergeEntityDetailsInput(BaseModel):\n",
    "    gsl_id: str = Field(description=\"The unique GSL ID of the source entity to be merged.\")\n",
    "\n",
    "@tool(args_schema=MergeEntityDetailsInput)\n",
    "def get_entity_by_id(gsl_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetches the full details for a single entity by its unique GSL ID.\"\"\"\n",
    "    get_team_query = f\"\"\"\n",
    "        query getTeams {{\n",
    "          searchableTeams(query: [{{ field: ID, operator: EQUALS, values: [\"{gsl_id}\"] }}]) {{\n",
    "            items {{ id name sport gender teamMembers {{ individual {{ id commonName {{ fullName }} }} }} competitions {{ id name }} }}\n",
    "          }}\n",
    "        }}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(GRAPHQL_ENDPOINT, headers=HEADERS, data=json.dumps({\"query\": get_team_query}))\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if \"errors\" in data:\n",
    "            return {\"error\": \"GraphQL query failed.\", \"details\": data['errors']}\n",
    "        \n",
    "        items = data.get(\"data\", {}).get(\"searchableTeams\", {}).get(\"items\", [])\n",
    "        return items[0] if items else {\"error\": f\"No entity found with GSL ID {gsl_id}\"}\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": f\"API call failed: {e}\"}\n",
    "    except (json.JSONDecodeError, IndexError):\n",
    "        return {\"error\": \"Failed to parse API response or find entity.\"}\n",
    "\n",
    "\n",
    "# --- Tool 2: Search for Matching Entities ---\n",
    "class TargetEntityDetailsInput(BaseModel):\n",
    "    search_term: str = Field(description=\"A name or keyword to search for matching entities.\")\n",
    "    \n",
    "@tool(args_schema=TargetEntityDetailsInput)\n",
    "def find_matching_entities(search_term: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Searches for entities by a name or keyword and returns a list of potential matches.\"\"\"\n",
    "    get_teams_query = f\"\"\"\n",
    "        query searchTeamsByName {{\n",
    "          searchableTeams(query: [{{ field: NAME, operator: CONTAINS, values: [\"{search_term}\"] }}]) {{\n",
    "            items {{ id name sport gender }}\n",
    "          }}\n",
    "        }}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(GRAPHQL_ENDPOINT, headers=HEADERS, data=json.dumps({\"query\": get_teams_query}))\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if \"errors\" in data:\n",
    "            print(f\"GraphQL API returned an error: {data['errors']}\")\n",
    "            return []\n",
    "        \n",
    "        return data.get(\"data\", {}).get(\"searchableTeams\", {}).get(\"items\", [])\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API call failed: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse API response.\")\n",
    "        return []\n",
    "\n",
    "# --- Tool 3: The Google Search Tool ---\n",
    "tavily_tool = TavilySearchResults(max_results=3)\n",
    "tavily_tool.name = \"tavily_search_results_json\"\n",
    "\n",
    "\n",
    "# --- Step 2: Create the Agent with a Modified Prompt for Parsing ---\n",
    "# The list of tools\n",
    "tools = [get_entity_by_id, find_matching_entities, tavily_tool]\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "# This is the core prompt - the \"brain\" of the agent.\n",
    "# It tells the agent HOW to perform the comparison task step-by-step.\n",
    "BATCH_PROCESSING_PROMPT = \"\"\"\n",
    "You are a highly meticulous data analyst specializing in entity resolution. Your goal is to combine internal API data with external information from a Google search to find the most accurate merge candidate for a source entity.\n",
    "\n",
    "**Your Comprehensive and Strict Analysis Protocol:**\n",
    "\n",
    "1.  **Get Source Data:** First, use the `get_entity_by_id` tool to fetch the complete data for the source entity using its GSL ID. This gives you the source entity's details, including its own ID.\n",
    "\n",
    "2.  **Conduct External Research:** After retrieving the source entity's data, use the `tavily_search_results_json` tool to perform a Google search using the source entity's name to gather external context.\n",
    "\n",
    "3.  **Find Internal Candidates:** Next, execute your internal search protocol. Perform at least two distinct searches using the `find_matching_entities` tool (e.g., by the entity's name and by a competition name) to gather a list of potential merge candidates.\n",
    "\n",
    "4.  **Consolidate and Deduplicate:** Combine the results from all your internal searches into a single, consolidated list of unique candidate entities based on their ID.\n",
    "\n",
    "5.  **CRITICAL - Filter Self-Matches:** This is a mandatory step. Before you begin your final analysis, you MUST filter the original source entity out of your consolidated list of candidates. Compare the `id` of each candidate to the `id` of the source entity you fetched in Step 1. Remove any candidate where the IDs are an exact match. **It is a critical error to suggest merging an entity with itself.**\n",
    "\n",
    "6.  **Synthesize and Conclude:** Now, with a clean list of *other* entities, perform your final analysis. Use all the information you have gathered (source data, external context, and filtered candidate data) to make your recommendation.\n",
    "\n",
    "**CRITICAL: Final Answer Formatting**\n",
    "Your final answer MUST be in the following format. First, provide the ID of the best match on its own line. Then, provide your justification.\n",
    "- If a good match is found, format it like this:\n",
    "Best Match ID: [GSL_ID_of_the_best_match]\n",
    "Justification: [Your detailed reasoning here...]\n",
    "\n",
    "- If no suitable match is found after your research, format it like this:\n",
    "Best Match ID: no match found\n",
    "Justification: [Your reasoning for why no match was found...]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", BATCH_PROCESSING_PROMPT),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the agent and executor (remains the same)\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# --- Step 3: Main Batch Processing Logic ---\n",
    "\n",
    "def parse_agent_output(output_text: str) -> (str, str):\n",
    "    \"\"\"Parses the agent's final answer to extract the best match ID and justification.\"\"\"\n",
    "    match_id = \"no match found\"\n",
    "    justification = output_text # Default to the full output\n",
    "\n",
    "    # Use regex to find the Best Match ID line\n",
    "    match = re.search(r\"Best Match ID:\\s*(.*)\", output_text)\n",
    "    if match:\n",
    "        match_id = match.group(1).strip()\n",
    "    \n",
    "    # Extract justification text that comes after the ID line\n",
    "    justification_split = output_text.split(\"Justification:\", 1)\n",
    "    if len(justification_split) > 1:\n",
    "        justification = justification_split[1].strip()\n",
    "\n",
    "    return match_id, justification\n",
    "\n",
    "def run_batch_process(input_file: str, output_file: str):\n",
    "    \"\"\"Reads source IDs from a CSV, processes them with the agent, and writes results to a new CSV.\"\"\"\n",
    "    print(f\"Starting batch process from '{input_file}'...\")\n",
    "    \n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Write header to the output file\n",
    "        writer.writerow([\"source_gsl_id\", \"best_match_gsl_id\", \"justification\"])\n",
    "        \n",
    "        # Skip header row of the input file\n",
    "        header = next(reader)\n",
    "        print(f\"Processing input file with header: {header}\")\n",
    "\n",
    "        for row in reader:\n",
    "            source_id = row[0]\n",
    "            print(f\"\\n--- Processing Source GSL ID: {source_id} ---\")\n",
    "\n",
    "            user_input = f\"Please research and compare the entity with GSL ID {source_id} to find the best merge candidate. Follow your instructions precisely.\"\n",
    "            \n",
    "            try:\n",
    "                response = agent_executor.invoke({\"input\": user_input})\n",
    "                output_data = response.get(\"output\", [])\n",
    "                output_text = \"\" # Default to empty string\n",
    "\n",
    "                # The response is a list like [{'type': 'text', 'text': '...'}].\n",
    "                # We need to safely extract the 'text' value.\n",
    "                if isinstance(output_data, list) and output_data:\n",
    "                    if isinstance(output_data[0], dict) and 'text' in output_data[0]:\n",
    "                        output_text = output_data[0]['text']\n",
    "                elif isinstance(output_data, str): # Handle if it's already a string\n",
    "                    output_text = output_data\n",
    "                \n",
    "                # Parse the structured output\n",
    "                best_match_id, justification = parse_agent_output(output_text)\n",
    "\n",
    "                print(f\"--- Result for {source_id}: Found match '{best_match_id}' ---\")\n",
    "                writer.writerow([source_id, best_match_id, justification])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!! An error occurred while processing {source_id}: {e} !!\")\n",
    "                writer.writerow([source_id, \"processing error\", str(e)])\n",
    "\n",
    "    print(f\"\\nBatch process complete. Results saved to '{output_file}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define input and output file names\n",
    "    input_csv_file = \"\"\n",
    "    output_csv_file = \"\"\n",
    "\n",
    "    # Run the main process\n",
    "    run_batch_process(input_file=input_csv_file, output_file=output_csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
